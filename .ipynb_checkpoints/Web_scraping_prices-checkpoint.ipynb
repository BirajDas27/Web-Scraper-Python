{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d7cf531-187a-426c-bf91-06adcf989048",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 91\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# ---------- Main Program ----------\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 91\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mðŸ” Enter product to search: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mâ³ Scraping Flipkart...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     94\u001b[0m     flipkart_results \u001b[38;5;241m=\u001b[39m scrape_flipkart(query)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py:1191\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1189\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1196\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py:1234\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1232\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1233\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1235\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1236\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "THIS CODE MOSTLY WILL NOT FIND DATA BECAUSE FLIPKART AND AMAZON USES SOME ANTI SCRAPING PROTECTIONS AND THEY KEEP CHANGING THE HTML\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                  \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                  \"Chrome/117.0.0.0 Safari/537.36\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "}\n",
    "\n",
    "# ---------- Flipkart Scraper ----------\n",
    "def scrape_flipkart(search_query):\n",
    "    query = search_query.replace(\" \", \"+\")\n",
    "    url = f\"https://www.flipkart.com/search?q={query}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    products = []\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        for item in soup.find_all(\"div\", {\"class\": \"_1AtVbE\"}):\n",
    "            # Flipkart product selectors (varies by category)\n",
    "            title = item.find(\"div\", {\"class\": \"_4rR01T\"}) or item.find(\"a\", {\"class\": \"s1Q9rs\"})\n",
    "            price = item.find(\"div\", {\"class\": \"_30jeq3\"})\n",
    "            rating = item.find(\"div\", {\"class\": \"_3LWZlK\"})\n",
    "            link = item.find(\"a\", {\"class\": \"_1fQZEK\"}) or item.find(\"a\", {\"class\": \"s1Q9rs\"})\n",
    "\n",
    "            if title and price and link:\n",
    "                products.append({\n",
    "                    \"Platform\": \"Flipkart\",\n",
    "                    \"Title\": title.text.strip(),\n",
    "                    \"Price\": price.text.strip(),\n",
    "                    \"Rating\": rating.text.strip() if rating else \"N/A\",\n",
    "                    \"Link\": \"https://www.flipkart.com\" + link[\"href\"]\n",
    "                })\n",
    "    return products\n",
    "\n",
    "\n",
    "# ---------- Amazon Scraper ----------\n",
    "def scrape_amazon(search_query):\n",
    "    query = search_query.replace(\" \", \"+\")\n",
    "    url = f\"https://www.amazon.in/s?k={query}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    products = []\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        for item in soup.find_all(\"div\", {\"data-component-type\": \"s-search-result\"}):\n",
    "            title_tag = item.h2.a.span if item.h2 and item.h2.a and item.h2.a.span else None\n",
    "            title = title_tag.text.strip() if title_tag else None\n",
    "\n",
    "            price_tag = item.find(\"span\", {\"class\": \"a-price-whole\"})\n",
    "            price = \"â‚¹\" + price_tag.text.strip() if price_tag else \"N/A\"\n",
    "\n",
    "            rating_tag = item.find(\"span\", {\"class\": \"a-icon-alt\"})\n",
    "            rating = rating_tag.text.strip() if rating_tag else \"N/A\"\n",
    "\n",
    "            link_tag = item.h2.a[\"href\"] if item.h2 and item.h2.a else None\n",
    "            link = \"https://www.amazon.in\" + link_tag if link_tag else \"N/A\"\n",
    "\n",
    "            if title:\n",
    "                products.append({\n",
    "                    \"Platform\": \"Amazon\",\n",
    "                    \"Title\": title,\n",
    "                    \"Price\": price,\n",
    "                    \"Rating\": rating,\n",
    "                    \"Link\": link\n",
    "                })\n",
    "    return products\n",
    "\n",
    "\n",
    "# ---------- Save to CSV ----------\n",
    "def save_to_csv(products, filename=\"products_comparison.csv\"):\n",
    "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"Platform\", \"Title\", \"Price\", \"Rating\", \"Link\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(products)\n",
    "    print(f\"âœ… Data saved to {filename}\")\n",
    "\n",
    "\n",
    "# ---------- Main Program ----------\n",
    "if __name__ == \"__main__\":\n",
    "    query = input(\"ðŸ” Enter product to search: \")\n",
    "\n",
    "    print(\"\\nâ³ Scraping Flipkart...\")\n",
    "    flipkart_results = scrape_flipkart(query)\n",
    "\n",
    "    print(\"â³ Scraping Amazon...\")\n",
    "    amazon_results = scrape_amazon(query)\n",
    "\n",
    "    all_results = flipkart_results + amazon_results\n",
    "\n",
    "    if all_results:\n",
    "        print(f\"\\nâœ… Found {len(all_results)} products\\n\")\n",
    "\n",
    "        for idx, p in enumerate(all_results, start=1):\n",
    "            print(f\"{idx}. [{p['Platform']}] {p['Title']}\")\n",
    "            print(f\"   Price  : {p['Price']}\")\n",
    "            print(f\"   Rating : {p['Rating']}\")\n",
    "            print(f\"   Link   : {p['Link']}\\n\")\n",
    "\n",
    "        save_to_csv(all_results)\n",
    "    else:\n",
    "        print(\"âš  No products found on Flipkart or Amazon.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d27e1042-b2ef-4ddc-8934-d3bc2ad8f0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1: http://books.toscrape.com/\n",
      "Scraping page 2: http://books.toscrape.com/catalogue/page-2.html\n",
      "Scraping page 3: http://books.toscrape.com/catalogue/page-3.html\n",
      "Scraping page 4: http://books.toscrape.com/catalogue/page-4.html\n",
      "Scraping page 5: http://books.toscrape.com/catalogue/page-5.html\n",
      "Scraping page 6: http://books.toscrape.com/catalogue/page-6.html\n",
      "Scraping page 7: http://books.toscrape.com/catalogue/page-7.html\n",
      "Scraping page 8: http://books.toscrape.com/catalogue/page-8.html\n",
      "Scraping page 9: http://books.toscrape.com/catalogue/page-9.html\n",
      "Scraping page 10: http://books.toscrape.com/catalogue/page-10.html\n",
      "Scraping page 11: http://books.toscrape.com/catalogue/page-11.html\n",
      "Scraping page 12: http://books.toscrape.com/catalogue/page-12.html\n",
      "Scraping page 13: http://books.toscrape.com/catalogue/page-13.html\n",
      "Scraping page 14: http://books.toscrape.com/catalogue/page-14.html\n",
      "Scraping page 15: http://books.toscrape.com/catalogue/page-15.html\n",
      "Scraping page 16: http://books.toscrape.com/catalogue/page-16.html\n",
      "Scraping page 17: http://books.toscrape.com/catalogue/page-17.html\n",
      "Scraping page 18: http://books.toscrape.com/catalogue/page-18.html\n",
      "Scraping page 19: http://books.toscrape.com/catalogue/page-19.html\n",
      "Scraping page 20: http://books.toscrape.com/catalogue/page-20.html\n",
      "Scraping page 21: http://books.toscrape.com/catalogue/page-21.html\n",
      "Scraping page 22: http://books.toscrape.com/catalogue/page-22.html\n",
      "Scraping page 23: http://books.toscrape.com/catalogue/page-23.html\n",
      "Scraping page 24: http://books.toscrape.com/catalogue/page-24.html\n",
      "Scraping page 25: http://books.toscrape.com/catalogue/page-25.html\n",
      "Scraping page 26: http://books.toscrape.com/catalogue/page-26.html\n",
      "Scraping page 27: http://books.toscrape.com/catalogue/page-27.html\n",
      "Scraping page 28: http://books.toscrape.com/catalogue/page-28.html\n",
      "Scraping page 29: http://books.toscrape.com/catalogue/page-29.html\n",
      "Scraping page 30: http://books.toscrape.com/catalogue/page-30.html\n",
      "Scraping page 31: http://books.toscrape.com/catalogue/page-31.html\n",
      "Scraping page 32: http://books.toscrape.com/catalogue/page-32.html\n",
      "Scraping page 33: http://books.toscrape.com/catalogue/page-33.html\n",
      "Scraping page 34: http://books.toscrape.com/catalogue/page-34.html\n",
      "Scraping page 35: http://books.toscrape.com/catalogue/page-35.html\n",
      "Scraping page 36: http://books.toscrape.com/catalogue/page-36.html\n",
      "Scraping page 37: http://books.toscrape.com/catalogue/page-37.html\n",
      "Scraping page 38: http://books.toscrape.com/catalogue/page-38.html\n",
      "Scraping page 39: http://books.toscrape.com/catalogue/page-39.html\n",
      "Scraping page 40: http://books.toscrape.com/catalogue/page-40.html\n",
      "Scraping page 41: http://books.toscrape.com/catalogue/page-41.html\n",
      "Scraping page 42: http://books.toscrape.com/catalogue/page-42.html\n",
      "Scraping page 43: http://books.toscrape.com/catalogue/page-43.html\n",
      "Scraping page 44: http://books.toscrape.com/catalogue/page-44.html\n",
      "Scraping page 45: http://books.toscrape.com/catalogue/page-45.html\n",
      "Scraping page 46: http://books.toscrape.com/catalogue/page-46.html\n",
      "Scraping page 47: http://books.toscrape.com/catalogue/page-47.html\n",
      "Scraping page 48: http://books.toscrape.com/catalogue/page-48.html\n",
      "Scraping page 49: http://books.toscrape.com/catalogue/page-49.html\n",
      "Scraping page 50: http://books.toscrape.com/catalogue/page-50.html\n",
      "\n",
      "âœ… Saved 1000 books to books_all.csv\n",
      "1. A Light in the Attic â€” Ã‚Â£51.77 â€” Rating: 3\n",
      "2. Tipping the Velvet â€” Ã‚Â£53.74 â€” Rating: 1\n",
      "3. Soumission â€” Ã‚Â£50.10 â€” Rating: 1\n",
      "4. Sharp Objects â€” Ã‚Â£47.82 â€” Rating: 4\n",
      "5. Sapiens: A Brief History of Humankind â€” Ã‚Â£54.23 â€” Rating: 5\n"
     ]
    }
   ],
   "source": [
    "# books_scraper.py\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import csv\n",
    "import time\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                   \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                   \"Chrome/117.0.0.0 Safari/537.36\"),\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\"\n",
    "}\n",
    "\n",
    "def rating_to_int(class_list):\n",
    "    # 'star-rating Three' -> 3\n",
    "    mapping = {\"One\": 1, \"Two\": 2, \"Three\": 3, \"Four\": 4, \"Five\": 5}\n",
    "    for c in class_list:\n",
    "        if c in mapping:\n",
    "            return mapping[c]\n",
    "    return None\n",
    "\n",
    "def scrape_books(base_url=\"http://books.toscrape.com/\", max_pages=None, delay=0.8, output_csv=\"books.csv\"):\n",
    "    url = base_url\n",
    "    page = 1\n",
    "    results = []\n",
    "\n",
    "    while True:\n",
    "        print(f\"Scraping page {page}: {url}\")\n",
    "        resp = requests.get(url, headers=HEADERS, timeout=15)\n",
    "        resp.raise_for_status()\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "        # Each product is in article.product_pod\n",
    "        for item in soup.select(\"article.product_pod\"):\n",
    "            try:\n",
    "                title_tag = item.h3.a\n",
    "                title = title_tag[\"title\"].strip() if title_tag and title_tag.has_attr(\"title\") else title_tag.text.strip()\n",
    "\n",
    "                rel_link = title_tag[\"href\"]\n",
    "                product_link = urljoin(url, rel_link)\n",
    "\n",
    "                price_tag = item.select_one(\"p.price_color\")\n",
    "                price = price_tag.text.strip() if price_tag else \"N/A\"\n",
    "\n",
    "                avail_tag = item.select_one(\"p.instock.availability\")\n",
    "                availability = \" \".join(avail_tag.text.split()) if avail_tag else \"N/A\"\n",
    "\n",
    "                rating_p = item.select_one(\"p.star-rating\")\n",
    "                rating = rating_to_int(rating_p.get(\"class\", [])) if rating_p else None\n",
    "\n",
    "                results.append({\n",
    "                    \"Title\": title,\n",
    "                    \"Price\": price,\n",
    "                    \"Availability\": availability,\n",
    "                    \"Rating\": rating,\n",
    "                    \"Link\": product_link\n",
    "                })\n",
    "            except Exception as e:\n",
    "                # skip problematic items but continue\n",
    "                print(\"  âš  Skipped an item due to:\", e)\n",
    "                continue\n",
    "\n",
    "        # Find next page link\n",
    "        next_li = soup.select_one(\"li.next > a\")\n",
    "        if not next_li:\n",
    "            break\n",
    "\n",
    "        next_href = next_li[\"href\"]\n",
    "        url = urljoin(url, next_href)\n",
    "        page += 1\n",
    "        if max_pages and page > max_pages:\n",
    "            break\n",
    "        time.sleep(delay)\n",
    "\n",
    "    # Save to CSV\n",
    "    if results:\n",
    "        with open(output_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=[\"Title\", \"Price\", \"Availability\", \"Rating\", \"Link\"])\n",
    "            writer.writeheader()\n",
    "            writer.writerows(results)\n",
    "        print(f\"\\nâœ… Saved {len(results)} books to {output_csv}\")\n",
    "    else:\n",
    "        print(\"âš  No results scraped.\")\n",
    "\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example: set max_pages to 2 during dev to speed up testing\n",
    "    scraped = scrape_books(max_pages=None, delay=0.6, output_csv=\"books_all.csv\")\n",
    "    # print first 5\n",
    "    for i, r in enumerate(scraped[:5], start=1):\n",
    "        print(f\"{i}. {r['Title']} â€” {r['Price']} â€” Rating: {r['Rating']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff572c3e-d901-42e8-9d7e-6c277d17205c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
